name: "VGG16_pool4"
## ---------------------------------------------------------------------------------------------
## INPUT
## ---------------------------------------------------------------------------------------------

# Anchor database
layer {
  name: "data_neut"
  type: "Data"
  top: "data_neut"
  top: "sim_pos"
  # Mean must be substracted.
  transform_param { # This is the Nordland dataset mean. Could be changed to Imagenet mean (Channels are in BGR)
    mean_value: 104.968053883
    mean_value: 119.316329094
    mean_value: 112.631406523
  }
  data_param {
    source: "~/caffe/examples/nordland/nordland_train_neut_lmdb" # This should point to the lmdb database
    batch_size: 20
    backend: LMDB
  }
}

# Positive database
layer {
  name: "data_pos"
  type: "Data"
  top: "data_pos"
  top: "sim_pos_2"
  transform_param {
    mean_value: 104.968053883
    mean_value: 119.316329094
    mean_value: 112.631406523
  }
  data_param {
    source: "~/caffe/examples/nordland/nordland_train_pos_lmdb"
    batch_size: 20
    backend: LMDB
  }
}

# Negative database
layer {
  name: "data_neg"
  type: "Data"
  top: "data_neg"
  top: "sim_neg"
  transform_param {
    mean_value: 104.968053883
    mean_value: 119.316329094
    mean_value: 112.631406523
  }
  data_param {
    source: "~/caffe/examples/nordland/nordland_train_neg_lmdb"
    batch_size: 20
    backend: LMDB
  }
}


## ---------------------------------------------------------------------------------------------
## FIRST LAYER OF CONVOLUTION (conv-relu-conv-relu-pool) RAMA 1

layer {
  name: "conv1_1"
  type: "Convolution"
  bottom: "data_neut"
  top: "conv1_1"
  param {
    name: "conv1_1_w"
	lr_mult: 0 
	decay_mult: 0 
  }
  param {
    name: "conv1_1_b"
	lr_mult: 0
	decay_mult: 0
  }
  convolution_param {
    	num_output: 64
	pad: 1
	kernel_size: 3
  }
}

layer {
  bottom: "conv1_1"
  top: "conv1_1"
  name: "relu1_1"
  type: "ReLU"
}

layer {
  bottom: "conv1_1"
  top: "conv1_2"
  name: "conv1_2"
  type: "Convolution"
  param {
    name: "conv1_2_w"
    lr_mult: 0
    decay_mult: 0
  }
  param {
    name: "conv1_2_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
  }
}
layer {
  bottom: "conv1_2"
  top: "conv1_2"
  name: "relu1_2"
  type: "ReLU"
}
layer {
  bottom: "conv1_2"
  top: "pool1"
  name: "pool1"
  type: "Pooling"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}

## ---------------------------------------------------------------------------------------------
## FIRST LAYER OF CONVOLUTION (conv-relu-conv-relu-pool) RAMA 2

layer {
  bottom: "data_pos"
  top: "conv1_1_p"
  name: "conv1_1_p"
  type: "Convolution"
  param {
    name: "conv1_1_w"
    lr_mult: 0
    decay_mult: 0
  }
  param {
    name: "conv1_1_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
  }
}

layer {
  bottom: "conv1_1_p"
  top: "conv1_1_p"
  name: "relu1_1_p"
  type: "ReLU"
}

layer {
  bottom: "conv1_1_p"
  top: "conv1_2_p"
  name: "conv1_2_p"
  type: "Convolution"
  param {
    name: "conv1_2_w"
    lr_mult: 0
    decay_mult: 0
  }
  param {
    name: "conv1_2_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
  }
}
layer {
  bottom: "conv1_2_p"
  top: "conv1_2_p"
  name: "relu1_2_p"
  type: "ReLU"
}
layer {
  bottom: "conv1_2_p"
  top: "pool1_p"
  name: "pool1_p"
  type: "Pooling"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}


## ---------------------------------------------------------------------------------------------
## FIRST LAYER OF CONVOLUTION (conv-relu-conv-relu-pool) RAMA 3

layer {
  bottom: "data_neg"
  top: "conv1_1_n"
  name: "conv1_1_n"
  type: "Convolution"
  param {
    name: "conv1_1_w"
    lr_mult: 0
    decay_mult: 0
  }
  param {
    name: "conv1_1_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
  }
}

layer {
  bottom: "conv1_1_n"
  top: "conv1_1_n"
  name: "relu1_1_n"
  type: "ReLU"
}

layer {
  bottom: "conv1_1_n"
  top: "conv1_2_n"
  name: "conv1_2_n"
  type: "Convolution"
  param {
    name: "conv1_2_w"
    lr_mult: 0
    decay_mult: 0
  }
  param {
    name: "conv1_2_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
  }
}
layer {
  bottom: "conv1_2_n"
  top: "conv1_2_n"
  name: "relu1_2_n"
  type: "ReLU"
}
layer {
  bottom: "conv1_2_n"
  top: "pool1_n"
  name: "pool1_n"
  type: "Pooling"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}

## ---------------------------------------------------------------------------------------------
## SECOND LAYER OF CONVOLUTION (conv-relu-conv-relu-pool) RAMA 1

layer {
  bottom: "pool1"
  top: "conv2_1"
  name: "conv2_1"
  type: "Convolution"
  param {
    name: "conv2_1_w"
    lr_mult: 0
    decay_mult: 0
  }
  param {
    name: "conv2_1_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
  }
}

layer {
  bottom: "conv2_1"
  top: "conv2_1"
  name: "relu2_1"
  type: "ReLU"
}

layer {
  bottom: "conv2_1"
  top: "conv2_2"
  name: "conv2_2"
  type: "Convolution"
  param {
    name: "conv2_2_w"
    lr_mult: 0
    decay_mult: 0
  }
  param {
    name: "conv2_2_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
  }
}
layer {
  bottom: "conv2_2"
  top: "conv2_2"
  name: "relu2_2"
  type: "ReLU"
}
layer {
  bottom: "conv2_2"
  top: "pool2"
  name: "pool2"
  type: "Pooling"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}

## ---------------------------------------------------------------------------------------------
## SECOND LAYER OF CONVOLUTION (conv-relu-conv-relu-pool) RAMA 2

layer {
  bottom: "pool1_p"
  top: "conv2_1_p"
  name: "conv2_1_p"
  type: "Convolution"
  param {
    name: "conv2_1_w"
    lr_mult: 0
    decay_mult: 0
  }
  param {
    name: "conv2_1_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
  }
}

layer {
  bottom: "conv2_1_p"
  top: "conv2_1_p"
  name: "relu2_1_p"
  type: "ReLU"
}

layer {
  bottom: "conv2_1_p"
  top: "conv2_2_p"
  name: "conv2_2_p"
  type: "Convolution"
  param {
    name: "conv2_2_w"
    lr_mult: 0
    decay_mult: 0
  }
  param {
    name: "conv2_2_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
  }
}
layer {
  bottom: "conv2_2_p"
  top: "conv2_2_p"
  name: "relu2_2_p"
  type: "ReLU"
}
layer {
  bottom: "conv2_2_p"
  top: "pool2_p"
  name: "pool2_p"
  type: "Pooling"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}


## ---------------------------------------------------------------------------------------------
## SECOND LAYER OF CONVOLUTION (conv-relu-conv-relu-pool) RAMA 3

layer {
  bottom: "pool1_n"
  top: "conv2_1_n"
  name: "conv2_1_n"
  type: "Convolution"
  param {
    name: "conv2_1_w"
    lr_mult: 0
    decay_mult: 0
  }
  param {
    name: "conv2_1_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
  }
}

layer {
  bottom: "conv2_1_n"
  top: "conv2_1_n"
  name: "relu2_1_n"
  type: "ReLU"
}

layer {
  bottom: "conv2_1_n"
  top: "conv2_2_n"
  name: "conv2_2_n"
  type: "Convolution"
  param {
    name: "conv2_2_w"
    lr_mult: 0
    decay_mult: 0
  }
  param {
    name: "conv2_2_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
  }
}
layer {
  bottom: "conv2_2_n"
  top: "conv2_2_n"
  name: "relu2_2_n"
  type: "ReLU"
}
layer {
  bottom: "conv2_2_n"
  top: "pool2_n"
  name: "pool2_n"
  type: "Pooling"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}

## ---------------------------------------------------------------------------------------------
## THIRD LAYER OF CONVOLUTION (conv-relu-conv-relu-conv-relu-pool) RAMA 1

layer {
  bottom: "pool2"
  top: "conv3_1"
  name: "conv3_1"
  type: "Convolution"
  param {
    name: "conv3_1_w"
    lr_mult: 0
    decay_mult: 0
  }
  param {
    name: "conv3_1_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
  }
}
layer {
  bottom: "conv3_1"
  top: "conv3_1"
  name: "relu3_1"
  type: "ReLU"
}
layer {
  bottom: "conv3_1"
  top: "conv3_2"
  name: "conv3_2"
  type: "Convolution"
  param {
    name: "conv3_2_w"
    lr_mult: 0
    decay_mult: 0
  }
  param {
    name: "conv3_2_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
  }
}
layer {
  bottom: "conv3_2"
  top: "conv3_2"
  name: "relu3_2"
  type: "ReLU"
}
layer {
  bottom: "conv3_2"
  top: "conv3_3"
  name: "conv3_3"
  type: "Convolution"
  param {
    name: "conv3_3_w"
    lr_mult: 0
    decay_mult: 0
  }
  param {
    name: "conv3_3_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
  }
}
layer {
  bottom: "conv3_3"
  top: "conv3_3"
  name: "relu3_3"
  type: "ReLU"
}
layer {
  bottom: "conv3_3"
  top: "pool3"
  name: "pool3"
  type: "Pooling"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}


## ---------------------------------------------------------------------------------------------
## THIRD LAYER OF CONVOLUTION (conv-relu-conv-relu-conv-relu-pool) RAMA 2

layer {
  bottom: "pool2_p"
  top: "conv3_1_p"
  name: "conv3_1_p"
  type: "Convolution"
  param {
    name: "conv3_1_w"
    lr_mult: 0
    decay_mult: 0
  }
  param {
    name: "conv3_1_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
  }
}
layer {
  bottom: "conv3_1_p"
  top: "conv3_1_p"
  name: "relu3_1_p"
  type: "ReLU"
}
layer {
  bottom: "conv3_1_p"
  top: "conv3_2_p"
  name: "conv3_2_p"
  type: "Convolution"
  param {
    name: "conv3_2_w"
    lr_mult: 0
    decay_mult: 0
  }
  param {
    name: "conv3_2_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
  }
}
layer {
  bottom: "conv3_2_p"
  top: "conv3_2_p"
  name: "relu3_2_p"
  type: "ReLU"
}
layer {
  bottom: "conv3_2_p"
  top: "conv3_3_p"
  name: "conv3_3_p"
  type: "Convolution"
  param {
    name: "conv3_3_w"
    lr_mult: 0
    decay_mult: 0
  }
  param {
    name: "conv3_3_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
  }
}
layer {
  bottom: "conv3_3_p"
  top: "conv3_3_p"
  name: "relu3_3_p"
  type: "ReLU"
}
layer {
  bottom: "conv3_3_p"
  top: "pool3_p"
  name: "pool3_p"
  type: "Pooling"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}


## ---------------------------------------------------------------------------------------------
## THIRD LAYER OF CONVOLUTION (conv-relu-conv-relu-conv-relu-pool) RAMA 3

layer {
  bottom: "pool2_n"
  top: "conv3_1_n"
  name: "conv3_1_n"
  type: "Convolution"
  param {
    name: "conv3_1_w"
    lr_mult: 0
    decay_mult: 0
  }
  param {
    name: "conv3_1_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
  }
}
layer {
  bottom: "conv3_1_n"
  top: "conv3_1_n"
  name: "relu3_1_n"
  type: "ReLU"
}
layer {
  bottom: "conv3_1_n"
  top: "conv3_2_n"
  name: "conv3_2_n"
  type: "Convolution"
  param {
    name: "conv3_2_w"
    lr_mult: 0
    decay_mult: 0
  }
  param {
    name: "conv3_2_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
  }
}
layer {
  bottom: "conv3_2_n"
  top: "conv3_2_n"
  name: "relu3_2_n"
  type: "ReLU"
}
layer {
  bottom: "conv3_2_n"
  top: "conv3_3_n"
  name: "conv3_3_n"
  type: "Convolution"
  param {
    name: "conv3_3_w"
    lr_mult: 0
    decay_mult: 0
  }
  param {
    name: "conv3_3_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
  }
}
layer {
  bottom: "conv3_3_n"
  top: "conv3_3_n"
  name: "relu3_3_n"
  type: "ReLU"
}
layer {
  bottom: "conv3_3_n"
  top: "pool3_n"
  name: "pool3_n"
  type: "Pooling"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}

## ---------------------------------------------------------------------------------------------
## FOURTH LAYER OF CONVOLUTION (conv-relu-conv-relu-conv-relu-pool) RAMA 1

layer {
  bottom: "pool3"
  top: "conv4_1"
  name: "conv4_1"
  type: "Convolution"
  param {
    name: "conv4_1_w"
    lr_mult: 0
    decay_mult: 0
  }
  param {
    name: "conv4_1_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
  }
}

layer {
  bottom: "conv4_1"
  top: "conv4_1"
  name: "relu4_1"
  type: "ReLU"
}

layer {
  bottom: "conv4_1"
  top: "conv4_2"
  name: "conv4_2"
  type: "Convolution"
  param {
    name: "conv4_2_w"
    lr_mult: 0
    decay_mult: 0
  }
  param {
    name: "conv4_2_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
  }
}

layer {
  bottom: "conv4_2"
  top: "conv4_2"
  name: "relu4_2"
  type: "ReLU"
}

layer {
  bottom: "conv4_2"
  top: "conv4_3"
  name: "conv4_3"
  type: "Convolution"
  param {
    name: "conv4_3_w"
    lr_mult: 0
    decay_mult: 0
  }
  param {
    name: "conv4_3_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
  }
}

layer {
  bottom: "conv4_3"
  top: "conv4_3"
  name: "relu4_3"
  type: "ReLU"
}

layer {
  bottom: "conv4_3"
  top: "pool4"
  name: "pool4"
  type: "Pooling"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}

layer {
  name: "feat"
  type: "InnerProduct"
  bottom: "pool4"
  top: "feat"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 128
    weight_filler {
      # type: "xavier"
      type: "gaussian" # initialize the filters from a Gaussian
      std: 1e-5
    }
    bias_filler {
      type: "constant"
    }
  }
}


## ---------------------------------------------------------------------------------------------
## FOURTH LAYER OF CONVOLUTION (conv-relu-conv-relu-conv-relu-pool) RAMA 2

layer {
  bottom: "pool3_p"
  top: "conv4_1_p"
  name: "conv4_1_p"
  type: "Convolution"
  param {
    name: "conv4_1_w"
    lr_mult: 0
    decay_mult: 0
  }
  param {
    name: "conv4_1_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
  }
}

layer {
  bottom: "conv4_1_p"
  top: "conv4_1_p"
  name: "relu4_1_p"
  type: "ReLU"
}

layer {
  bottom: "conv4_1_p"
  top: "conv4_2_p"
  name: "conv4_2_p"
  type: "Convolution"
  param {
    name: "conv4_2_w"
    lr_mult: 0
    decay_mult: 0
  }
  param {
    name: "conv4_2_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
  }
}

layer {
  bottom: "conv4_2_p"
  top: "conv4_2_p"
  name: "relu4_2_p"
  type: "ReLU"
}

layer {
  bottom: "conv4_2_p"
  top: "conv4_3_p"
  name: "conv4_3_p"
  type: "Convolution"
  param {
    name: "conv4_3_w"
    lr_mult: 0
    decay_mult: 0
  }
  param {
    name: "conv4_3_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
  }
}

layer {
  bottom: "conv4_3_p"
  top: "conv4_3_p"
  name: "relu4_3_p"
  type: "ReLU"
}

layer {
  bottom: "conv4_3_p"
  top: "pool4_p"
  name: "pool4_p"
  type: "Pooling"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}

layer {
  name: "feat_p"
  type: "InnerProduct"
  bottom: "pool4_p"
  top: "feat_p"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 128
    weight_filler {
      # type: "xavier"
       type: "gaussian" # initialize the filters from a Gaussian
       std: 1e-5
    }
    bias_filler {
      type: "constant"
    }
  }
}


## ---------------------------------------------------------------------------------------------
## FOURTH LAYER OF CONVOLUTION (conv-relu-conv-relu-conv-relu-pool) RAMA 3

layer {
  bottom: "pool3_n"
  top: "conv4_1_n"
  name: "conv4_1_n"
  type: "Convolution"
  param {
    name: "conv4_1_w"
    lr_mult: 0
    decay_mult: 0
  }
  param {
    name: "conv4_1_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
  }
}

layer {
  bottom: "conv4_1_n"
  top: "conv4_1_n"
  name: "relu4_1_n"
  type: "ReLU"
}

layer {
  bottom: "conv4_1_n"
  top: "conv4_2_n"
  name: "conv4_2_n"
  type: "Convolution"
  param {
    name: "conv4_2_w"
    lr_mult: 0
    decay_mult: 0
  }
  param {
    name: "conv4_2_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
  }
}

layer {
  bottom: "conv4_2_n"
  top: "conv4_2_n"
  name: "relu4_2_n"
  type: "ReLU"
}

layer {
  bottom: "conv4_2_n"
  top: "conv4_3_n"
  name: "conv4_3_n"
  type: "Convolution"
  param {
    name: "conv4_3_w"
    lr_mult: 0
    decay_mult: 0
  }
  param {
    name: "conv4_3_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
  }
}

layer {
  bottom: "conv4_3_n"
  top: "conv4_3_n"
  name: "relu4_3_n"
  type: "ReLU"
}

layer {
  bottom: "conv4_3_n"
  top: "pool4_n"
  name: "pool4_n"
  type: "Pooling"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}

layer {
  name: "feat_n"
  type: "InnerProduct"
  bottom: "pool4_n"
  top: "feat_n"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 128
    weight_filler {
      # type: "xavier"
       type: "gaussian" # initialize the filters from a Gaussian
       std: 1e-5
    }
    bias_filler {
      type: "constant"
    }
  }
}


## --------------------------------------------------
## LOSS

## DISTANCIA POSITIVA 

layer {
  name: "pos_diff"
  type: "Eltwise"
  bottom: "feat"
  bottom: "feat_p"
  top: "pos_diff"
  eltwise_param {
    operation: SUM
    coeff: 1
    coeff: -1
  }
}

layer {
  name: "pos_diff_square"
  type: "Power"
  bottom: "pos_diff"
  top: "pos_diff_square"
  power_param {
    power: 2
    scale: 1
    shift: 0
  }
}


layer {
  name: "sum_pos"
  type: "Reduction"
  bottom: "pos_diff_square"
  top: "sum_pos"
  reduction_param {
    axis: 1 # reduce all dims after first
    operation: SUM  # use absolute sum
  }
}

layer {
  name: "duplicate_sumpos"
  type: "Split"
  bottom: "sum_pos"
  top: "sum_pos1"
  top: "sum_pos2"
}

layer {
  name: "dist_pos"
  type: "Power"
  bottom: "sum_pos1"
  top: "dist_pos"
  power_param {
    power: 0.5
    scale: 1
    shift: 0.0000001
  }
}

layer {
  name: "duplicate_distpos"
  type: "Split"
  bottom: "dist_pos"
  top: "dist_pos_1"
  top: "dist_pos_2"
}

## DISTANCIA NEGATIVA

layer {
  name: "neg_diff"
  type: "Eltwise"
  bottom: "feat"
  bottom: "feat_n"
  top: "neg_diff"
  eltwise_param {
    operation: SUM
    coeff: 1
    coeff: -1
  }
}

layer {
  name: "neg_diff_square"
  type: "Power"
  bottom: "neg_diff"
  top: "neg_diff_square"
  power_param {
    power: 2
    scale: 1
    shift: 0
  }
}

layer {
  name: "sum_neg"
  type: "Reduction"
  bottom: "neg_diff_square"
  top: "sum_neg"
  reduction_param {
    axis: 1 # reduce all dims after first
    operation: SUM  # use absolute sum
  }
}

layer {
  name: "dist_neg"
  type: "Power"
  bottom: "sum_neg"
  top: "dist_neg"
  power_param {
    power: 0.5
    scale: 1
    shift: 0.0000001
  }
}

layer {
  name: "duplicate_distneg"
  type: "Split"
  bottom: "dist_neg"
  top: "dist_neg_1"
  top: "dist_neg_2"
}

# SUMAR MARGEN E INVERSION DE PARTE POSITIVA

layer {
  name: "denom"
  type: "Power"
  bottom: "dist_pos_1"
  top: "denom"
  power_param {
    power: -1
    scale: 1
    shift: 1 # MARGEN
  }
}

# FRACCION DISTANCIA NEGATIVA Y POSITIVA CON MARGEN

layer {
  name: "relation_neg_pos"
  type: "Eltwise"
  bottom: "dist_neg_1"
  bottom: "denom"
  top: "relation_neg_pos"
  eltwise_param {
    operation: PROD
  }
}

layer {
  name: "duplicate_relation"
  type: "Split"
  bottom: "relation_neg_pos"
  top: "relation_neg_pos1"
  top: "relation_neg_pos2"
}

# 1 MENOS FRACCION 

layer {
  name: "shift_relation"
  type: "Power"
  bottom: "relation_neg_pos1"
  top: "shift_relation"
  power_param {
    power: 1
    scale: -1
    shift: 1 # MARGEN
  }
}

# BLOB MISMA DIMENSION CON CEROS 

layer {
  name: "duplicate_shift"
  type: "Split"
  bottom: "shift_relation"
  top: "shift_relation_1"
  top: "shift_relation_2"
}

layer {
  name: "ceros"
  type: "Power"
  bottom: "shift_relation_2"
  top: "ceros"
  power_param {
    power: 1
    scale: 0
    shift: 0 # MARGEN
  }
}

# MAXIMO ENTRE CERO Y RELACION

layer {
  name: "max_value"
  type: "Eltwise"
  bottom: "ceros"
  bottom: "shift_relation_1"
  top: "max_value"
  eltwise_param {
    operation: MAX
  }
}

layer {
  name: "duplicate_max"
  type: "Split"
  bottom: "max_value"
  top: "max_value1"
  top: "max_value2"
}

## LOSS 

layer {
  name: "loss"
  type: "Reduction"
  bottom: "max_value1"
  top: "loss"
  reduction_param {
    axis: 0 # reduce all dims after first
    operation: MEAN  # use mean
    coeff: 0.5 # 1/2N
  }
  loss_weight: 1 # makes this a loss layer
}








